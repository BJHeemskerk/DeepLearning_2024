{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BJHeemskerk/DeepLearning_2024/blob/Henry/Opdracht%201/Portfolio_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzNuCpg4hfjU"
      },
      "source": [
        "# **Voorspellen van huisprijzen**\n",
        "\n",
        "|Teamleden|Kaggle Username|GitHub Username|\n",
        "|--|--|--|\n",
        "|Nima Ghafar|NimaGhafar|NimaGhafar|\n",
        "|Busse Heemskerk|bussejheemskerk|BJHeemskerk|\n",
        "|Henry Lau|thugwithin|HenryLau08|\n",
        "|Jesse van Leeuwen|22096337|22096337|\n",
        "\n",
        "-- Opdrachtomschrijving --\n",
        "\n",
        "## Inhoudsopgave\n",
        "1. [H1: Importeren en inladen](#h1)\n",
        "2. [H2: EDA](#h2)\n",
        "3. [H3: Fully Connected Neural Network](#h3)\n",
        "\n",
        "## H1: Importeren en inladen <a name='h1'></a>\n",
        "Eerst zullen de verschillende gebruikte libaries worden geimporteerd."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Kj37F8qEhfjj"
      },
      "outputs": [],
      "source": [
        "# Importeren van veel gebruikte libaries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from PIL import Image\n",
        "import datetime\n",
        "# Importeren libaries voor aanpassen/behandelen features\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Importeren DL libaries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F279pjIRhfjq"
      },
      "source": [
        "Nu zal de data worden ingeladen. Om hiermee te beginnen zullen de csv bestanden worden ingeladen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWJuOikmieqI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRxMk8E4iqM5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/DeepLearning_2024/Opdracht 1')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1TR8DcJhfjs"
      },
      "outputs": [],
      "source": [
        "# Inladen van csv bestanden\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "# Tonen van de data\n",
        "display(train_data.head(), test_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7bDzylvhfjt"
      },
      "source": [
        "In de volgende code cel wordt de data van elke afbeelding in de train en de test map opgehaald."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBRpk-N4jZ03"
      },
      "outputs": [],
      "source": [
        "# Aanmaken van te vullen lijsten\n",
        "file_paths_tr = []\n",
        "imgdata_tr = []\n",
        "file_paths_te = []\n",
        "imgdata_te = []\n",
        "\n",
        "# Opzetten directories\n",
        "directories = [\"Train\", \"Test\"]\n",
        "\n",
        "# Ophalen van de bestanden\n",
        "# Loopen over directories\n",
        "for directory in directories:\n",
        "    directory_path = os.path.join('/content/drive/My Drive/Colab Notebooks/DeepLearning_2024/Opdracht 1', directory)\n",
        "    # Loopen over alle bestanden\n",
        "    for file in os.listdir(directory_path):\n",
        "        # Alleen checken van .jpg bestanden\n",
        "        if file.endswith(\".jpg\"):\n",
        "            # Aanmaken van pad naar het bestand.\n",
        "            file_path = os.path.join(directory_path, file)\n",
        "\n",
        "            # Openen van de afbeelding en omzetten naar np.array\n",
        "            img = Image.open(file_path)\n",
        "            img_array = np.array(img)\n",
        "\n",
        "            # Vullen van juiste lijsten op basis van ID\n",
        "            if int(file.split('.')[0]) > 500:  # We halen de ID uit de bestandsnaam\n",
        "                file_paths_te.append(file_path)\n",
        "                imgdata_te.append(img_array)\n",
        "            else:\n",
        "                file_paths_tr.append(file_path)\n",
        "                imgdata_tr.append(img_array)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFAhRAuihfju"
      },
      "outputs": [],
      "source": [
        "# Aanmaken van te vullen lijsten\n",
        "file_paths_tr = []\n",
        "imgdata_tr = []\n",
        "file_paths_te = []\n",
        "imgdata_te = []\n",
        "\n",
        "# Opzetten directories\n",
        "directories = [\"Train/\", \"Test/\"]\n",
        "\n",
        "# Ophalen van de bestanden\n",
        "# Loopen over directories\n",
        "for directory in directories:\n",
        "    # Loopen over alle bestanden\n",
        "    for file in os.listdir(directory):\n",
        "        # Alleen checken van .jpg bestanden\n",
        "        if file.endswith(\".jpg\"):\n",
        "            # Aanmaken van pad naar het bestand.\n",
        "            file_path = os.path.join(directory, file)\n",
        "\n",
        "            # Openen van de afbeelding en omzetten naar np.array\n",
        "            img = Image.open(file_path)\n",
        "            img_array = np.array(img)\n",
        "\n",
        "            # Ophalen numerieke ID van het beastand\n",
        "            file_number = ''.join(filter(str.isdigit, file))\n",
        "\n",
        "            # Vullen van juiste lijsten op basis van ID\n",
        "            if file_number and int(file_number) > 500:\n",
        "                file_paths_te.append(file_path)\n",
        "                imgdata_te.append(img_array)\n",
        "            else:\n",
        "                file_paths_tr.append(file_path)\n",
        "                imgdata_tr.append(img_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D02vJA4fhfjw"
      },
      "source": [
        "Nu alle data in lijsten is gezet kunnen er dataframes worden aangemaakt met de data van de afbeeldingen. Door de grootte van de arrays kan dit even duren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOcRpAjthfjy"
      },
      "outputs": [],
      "source": [
        "# Initieren van het dataframe\n",
        "train_images = pd.DataFrame(columns=[\"House ID\", \"FilePath\", \"ImgData\"])\n",
        "test_images = pd.DataFrame(columns=[\"House ID\", \"FilePath\", \"ImgData\"])\n",
        "\n",
        "# Aanvullen train dataset\n",
        "train_images[\"House ID\"] = train_data[\"House ID\"]\n",
        "train_images[\"FilePath\"] = file_paths_tr\n",
        "train_images[\"ImgData\"] = imgdata_tr\n",
        "\n",
        "# Aanvullen test dataset\n",
        "test_images[\"House ID\"] = test_data[\"House ID\"]\n",
        "test_images[\"FilePath\"] = file_paths_te\n",
        "test_images[\"ImgData\"] = imgdata_te\n",
        "\n",
        "# Tonen van de dataframes\n",
        "display(train_images.head(), test_images.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxfm7M2Zhfjz"
      },
      "source": [
        "Vervolgens zullen de dataframes worden getest door de afbeeldingen te tonen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzIEfqNAhfj1"
      },
      "outputs": [],
      "source": [
        "# Ophalen van pad naar afbeelding\n",
        "image_path = train_images[\"FilePath\"][0]\n",
        "\n",
        "# Openen afbeelding\n",
        "img = Image.open(image_path)\n",
        "\n",
        "# Omzetten afbeelding naar np.array\n",
        "img_array = np.array(img)\n",
        "\n",
        "# Tonen van de afbeelding\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "# Opnieuw tonen van de afbeelding via DF\n",
        "plt.imshow(train_images[\"ImgData\"][0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW2xcQMUhfj2"
      },
      "source": [
        "## H2: EDA <a name='h2'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcbrOlUfhfj3"
      },
      "source": [
        "- [ ] Voer een Exploratieve Data Analyse (EDA) uit:\n",
        "    - [ ] Lees de afbeeldingen in en toon enkele afbeeldingen.\n",
        "    - [ ] Laad het metadatabestand en maak relevante visualisaties om de relaties tussen de targetvariabele en de andere variabelen te onderzoeken.\n",
        "- [ ] Beschrijf wat de belangrijkste bevindingen zijn van de EDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEqi6YbDhfj4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the metadata\n",
        "train_df = pd.read_csv(r'train.csv')\n",
        "test_df = pd.read_csv(r'test.csv')\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "display(train_df.head())\n",
        "display(test_df.head())\n",
        "\n",
        "# Read and display images\n",
        "train_images_dir = r'Train'\n",
        "test_images_dir = r'Test'\n",
        "\n",
        "# Display some images from the train set\n",
        "train_images = os.listdir(train_images_dir)\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    img = cv2.imread(os.path.join(train_images_dir, train_images[i]))\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "\n",
        "# Display some images from the test set\n",
        "if 'Price' not in train_df.columns:\n",
        "    raise KeyError(\"Column 'target' does not exist in the dataframe.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg0YXjCRhfj6"
      },
      "outputs": [],
      "source": [
        "# EDA on train dataframe\n",
        "print(\"Train dataframe shape:\", train_df.shape)\n",
        "\n",
        "# Use describe() for summary statistics\n",
        "print(\"Train dataframe summary statistics:\")\n",
        "display(train_df.describe(include='all'))\n",
        "\n",
        "# Use info() for data types and missing values\n",
        "print(\"Train dataframe info:\")\n",
        "display(train_df.info())\n",
        "\n",
        "# EDA on test dataframe\n",
        "print(\"Test dataframe shape:\", test_df.shape)\n",
        "\n",
        "# Use describe() for summary statistics\n",
        "print(\"Test dataframe summary statistics:\")\n",
        "display(test_df.describe(include='all'))\n",
        "\n",
        "# Use info() for data types and missing values\n",
        "print(\"Test dataframe info:\")\n",
        "display(test_df.info())\n",
        "\n",
        "# Visualize the distribution of the target variable in the train dataframe\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(train_df['Price'], kde=True)\n",
        "plt.title('Distribution of Price in Train Data')\n",
        "plt.xlabel('Price')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "sns.pairplot(train_df)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At23Y2Bjhfj7"
      },
      "outputs": [],
      "source": [
        "# Maak een heatmap met  cmap='RdYlGn' alleen op \"Price\" in de train dataframe en in volgorde van hoog naar laag\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(train_df.corr()[['Price']].sort_values(by='Price', ascending=False), annot=True, cmap='RdYlGn')\n",
        "plt.title('Correlation of Features with Price')\n",
        "plt.show()\n",
        "\n",
        "# Maak een heatmap met  cmap='RdYlGn' voor alle kolommen in de train dataframe en in volgorde van hoog naar laag\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(train_df.corr(), annot=True, cmap='RdYlGn')\n",
        "plt.title('Correlation of Features')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj7K9LYthfj8"
      },
      "source": [
        "## H3: Splitsen en scalen\n",
        "\n",
        "Voordat er neurale netwerken kunnen worden opgesteld, zal eerst nog de train_data dataset worden opgesplitst in een test en een validatie set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFlvBx2khfj9"
      },
      "outputs": [],
      "source": [
        "# Aanmaken van de random_state SEED\n",
        "SEED = 42\n",
        "\n",
        "# Opslitsten van data in X en y\n",
        "X = train_data.drop(['House ID', 'Price'], axis=1)\n",
        "y = train_data['Price']\n",
        "\n",
        "# Toepassen van train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "                                        X, y, test_size=0.1, random_state=SEED\n",
        "                                       )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRDBdyxahfj-"
      },
      "source": [
        "Vervolgens moet de data worden gescaled, dit zorgt ervoor dat het model betere verbanden kan leggen tussen de verschillende variabelen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbsipE37hfj-"
      },
      "outputs": [],
      "source": [
        "# Scalen van de gesplitste data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Scalen van de test_data\n",
        "# Aanmaken lijst met nodige kolomnamen\n",
        "kolomnamen = [\"Bedrooms\", \"Bathrooms\", \"Area\", \"Latitude\", \"Longitude\"]\n",
        "\n",
        "# Scalen van de nodige kolommen\n",
        "test_fe = test_data[kolomnamen]\n",
        "sc_test_fe = scaler.fit_transform(test_fe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grhMLYX30WxV"
      },
      "source": [
        "**versie 1 van het model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuNwOujzhfj_"
      },
      "source": [
        "## H3: Fully Connencted Neural Network <a name='h3'></a>\n",
        "\n",
        "Nu kan er worden begonnen met het opzetten van verschillende voorspellingen. Het eerste type voorspelling dat voor zal komen is een Fully Connected Neural Network. Dit netwerk bestaat uit de volgende lagen:\n",
        "\n",
        "**Input laag:**<br>\n",
        "De input laag van een neuraal netwerk bestaat uit een bepaald aantal neuronen. Het aantal neuronen moet gelijk zijn aan het aantal features dat in de dataset beschikbaar is. In onze datasets zijn er 5 features te vinden, dus heeft de input laag 5 neuronen.\n",
        "\n",
        "In alle hidden layers is gebruik gemaakt van een trial & error aanpak, waarmee er met verschillende hoeveelheden van neuronen en verschillende activatie functies is getest. Uiteindelijk kwamen hier de volgende aantallen en activaties uit:\n",
        "\n",
        "**Hidden Layer 1:**<br>\n",
        "- Neuronen: 128\n",
        "- Activatie: Rectified Linear Unit\n",
        "\n",
        "**Hidden Layer 2:**<br>\n",
        "- Neuronen: 64\n",
        "- Activatie: Rectified Linear Unit\n",
        "\n",
        "**Hidden Layer 3:**<br>\n",
        "- Neuronen: 64\n",
        "- Activatie: Rectified Linear Unit\n",
        "\n",
        "**Hidden Layer 4:**<br>\n",
        "- Neuronen: 16\n",
        "- Activatie: Rectified Linear Unit\n",
        "\n",
        "Voor de output laag geldt ongeveer hetzelfde als voor de input laag. Het aantal neuronen in de output laag hoort gelijk te zijn aan het aantal klassen die worden voorspeld bij classificatie. Bij regressie taken is er vaak maar 1 neuron in de output laag aanwezig, omdat er een eenvoudige numerieke voorspelling wordt gemaakt. In ons regressie probleem is deze waarde de huisprijs. Bij regressie is er ook sprake van de activatie functie 'linear', ookwel bekend als geen activatie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27YT75hLhfkA"
      },
      "source": [
        "Nu de data is gesplitst zal er een Fully Connected Neural Network worden opgebouwd."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIdJe2E8hfkB"
      },
      "outputs": [],
      "source": [
        "# Aanmaken NN mbv Sequential API\n",
        "fcnn = Sequential()\n",
        "\n",
        "# Toevoegen input en eerste hidden layer\n",
        "fcnn.add(Dense(128, input_shape=(5,), activation='relu'))\n",
        "\n",
        "# Toevoegen andere hidden layers\n",
        "fcnn.add(Dense(64, activation='relu'))\n",
        "fcnn.add(Dense(64, activation='relu'))\n",
        "fcnn.add(Dense(16, activation='relu'))\n",
        "\n",
        "# Toevoegen van output layer\n",
        "fcnn.add(Dense(1, activation='linear'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKPyrUTThfkC"
      },
      "source": [
        "Nadat alle lagen van het model zijn opgesteld, is het tijd om het model te compilen. Deze stap voegt enkele parameters toe aan het model:\n",
        "- De optimizer, een algoritme dat bepaalt op welke wijze het model leert tijdens het trainen.\n",
        "- De loss functie, een functie die aantoont hoe goed het model presteert op de gegevens.\n",
        "- De metric, een extra, optionele meetwaarde voor de prestatie van het model.\n",
        "\n",
        "Voor ons model is er gekozen voor de volgende parameters:\n",
        "- De Adam (Adaptive Moment estimator) optimizer\n",
        "- Mean Squared Error als loss\n",
        "- Mean Absolute Percentage Error als metric.\n",
        "\n",
        "De keuze voor de optimizer en de loss is gemaakt door een paar van de bekendste en meest gebruikte opties te nemen. Als metric is er gekozen voor de MAPE, omdat deze metric ook wordt gebruikt in de kaggle competitie.\n",
        "\n",
        "Daarnaast is er ook een module aangemaakt om het model eerder te kunnen stoppen, in het geval dat er niet veel meer veranderd aan de loss van de validatie data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pokVw69ChfkD"
      },
      "outputs": [],
      "source": [
        "# Compilen van het model\n",
        "fcnn.compile(\n",
        "    optimizer=Adam(learning_rate=0.01),\n",
        "    loss=tf.keras.losses.MeanSquaredError(),\n",
        "    metrics=[tf.keras.metrics.MeanAbsolutePercentageError()]\n",
        ")\n",
        "\n",
        "# Maken van een vroege stop module\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss', patience=25, restore_best_weights=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2wPmCcPhfkE"
      },
      "source": [
        "Nu alles is aangemaakt kan het model worden getrained. Hierbij gebruiken we de trainings data, de validatie data, een gekozen aantal epochs, een batch size en een callback module. De epochs staan voor het aantal iteraties dat het model door zal gaan, door gebruik van de callback module is er gekozen om 5000 epochs uit te voeren. Zodra de loss op de validatie set voor 100 epochs niet genoeg veranderd zal het model automatisch stoppen en worden de parameters van de beste epoch hersteld. De batch size wijst op het aantal datapunten dat per epoch wordt gebruikt, bij ons is er gekozen voor een batch size van 16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHxX14LRhfkE"
      },
      "outputs": [],
      "source": [
        "# Fitten van het model met de data\n",
        "h_fcnn = fcnn.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=16,\n",
        "    epochs=5000,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stop]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz4AszUkhfkF"
      },
      "source": [
        "Met het getrainde model kan er gekeken worden naar de loss van de train data en de validatie data. Op deze manier kan er in een enkele grafiek worden gezien of er sprake is van overfitting of andere problemen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U-ymTxShfkG"
      },
      "outputs": [],
      "source": [
        "plt.plot(h_fcnn.history['loss'], label='Training loss')\n",
        "plt.plot(h_fcnn.history['val_loss'], 'g', label='Validation loss')\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('Loss (MAPE)')\n",
        "plt.title('Loss grafiek voor FCNN-model')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L98VZ0_ThfkH"
      },
      "source": [
        "Zoals in de grafiek is te zien, is er sprake van geen van overfitting. Overfitting wordt namelijk aangeduid door het stijgen van de validatie loss.\n",
        "\n",
        "Nu het model is getrained en de loss is bekeken, kan de voorspelling worden geupload naar kaggle. Aan de hand van de voorspellingen kan er een csv bestand aangemaakt worden voor de kaggle competitie. Omdat deze bestanden vaker moeten worden aangemaakt en om de bestanden overzichtelijk en uniform te houden is hiervoor een functie aangemaakt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57wEc5MYunxD"
      },
      "outputs": [],
      "source": [
        "model_save_path = '/content/drive/My Drive/Colab Notebooks/DeepLearning_2024/Opdracht 1/modelversies/fcnn_model_v1.h5'\n",
        "fcnn.save(model_save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdvCSZhlu3Pe"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "model_load_path = '/content/drive/My Drive/Colab Notebooks/DeepLearning_2024/Opdracht 1/modelversies/fcnn_model_v1.h5'\n",
        "\n",
        "\n",
        "model = load_model(model_load_path)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r9tRGzf0iGg"
      },
      "source": [
        "**versie 2 van het model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XO5S7k20mJn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# hier definieren we het modle\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer en eerste hidden layer met Dropout\n",
        "model.add(Dense(128, input_shape=(5,), activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Tweede hidden layer\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Derde hidden layer\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compileer het model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "\n",
        "# Train het model\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=1000,\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=[early_stop])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECnlsQnB4UL-"
      },
      "source": [
        "## Modelaanpassingen en Verbeteringen\n",
        "\n",
        "In deze versie van het neurale netwerkmodel zijn verschillende aanpassingen gemaakt ten opzichte van de eerste versie van het model. Deze wijzigingen zijn bedoeld om de modelprestaties te verbeteren, zoals gemeten door een lagere Mean Absolute Percentage Error (MAPE) op Kaggle. Hier volgt een overzicht van de belangrijkste aanpassingen:\n",
        "\n",
        "### Architectuur van het Model\n",
        "- **Verhoging van het Aantal Neuronen**: In elke laag van het model is het aantal neuronen aangepast om de modelcomplexiteit te verhogen. Dit stelt het model in staat om complexere patronen in de data te leren.\n",
        "    - De inputlaag en de eerste verborgen laag bevatten nu 128 neuronen.\n",
        "    - De tweede verborgen laag bevat 64 neuronen.\n",
        "    - De derde verborgen laag bevat 32 neuronen.\n",
        "- **Dropout Toegevoegd**: In elke verborgen laag is een dropout van 0.2 toegepast. Dropout helpt overfitting te verminderen door willekeurig een deel van de neuronen tijdens de training uit te schakelen, wat zorgt voor een robuuster netwerk.\n",
        "\n",
        "### Hyperparameters\n",
        "- **Learning Rate**: De learning rate van de Adam optimizer is ingesteld op 0.001. Dit is een standaardwaarde die vaak een goed evenwicht biedt tussen snelheid en nauwkeurigheid van de training.\n",
        "- **Batch Size**: De batch size is vastgesteld op 32. Dit is een veelgebruikte grootte die een goede balans biedt tussen de snelheid van de training en de nauwkeurigheid van de gradient updates.\n",
        "- **Aantal Epochs**: Er is gekozen voor een maximum van 1000 epochs, met early stopping toegepast om te voorkomen dat het model te lang traint en overfit raakt.\n",
        "\n",
        "### Training en Validatie\n",
        "- **Validation Split**: Een validatiesplit van 0.2 wordt gebruikt om een deel van de trainingsdata apart te zetten voor validatiedoeleinden. Dit helpt bij het monitoren van het modelprestaties op niet-getrainde data.\n",
        "- **Early Stopping**: Early stopping wordt gebruikt met een `patience` van 20 epochs. Dit betekent dat de training stopt als het validatieverlies voor 20 opeenvolgende epochs niet verbetert, waarbij de beste modelgewichten worden hersteld.\n",
        "\n",
        "Deze aanpassingen zijn gemaakt met het doel om de nauwkeurigheid van het model te verbeteren en overfitting te verminderen. Door het aantal neuronen te verhogen en dropout toe te passen, is het model beter in staat om de onderliggende patronen in de data te leren zonder te veel te leren van de ruis of specifieke details van de trainingsset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqK4Nytq6Oeb"
      },
      "outputs": [],
      "source": [
        "# Opslaan van het getrainde model (het tweede model) op de gespecificeerde locatie\n",
        "model_save_path = '/content/drive/My Drive/Colab Notebooks/DeepLearning_2024/Opdracht 1/modelversies/fcnn_model_v2.h5'\n",
        "model.save(model_save_path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4CU6aSQ6czs"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "model_load_path = '/content/drive/My Drive/Colab Notebooks/DeepLearning_2024/Opdracht 1/modelversies/fcnn_model_v2.h5'\n",
        "\n",
        "\n",
        "model = load_model(model_load_path)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUCWrfRNv_4f"
      },
      "source": [
        "Met die code heb ik het eerste model opgeslagen in een nieuwe map genaamd modelversies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut6Cd6rohfkH"
      },
      "outputs": [],
      "source": [
        "def kaggle_csv(model, input_, suffix):\n",
        "    \"\"\"\n",
        "    Deze functie maakt de voorspelling op de test dataset\n",
        "    en vormt deze om tot een csv bestand om in te kunnen\n",
        "    leveren op Kaggle.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    model : ML-model of DL-model\n",
        "        De naam die is gegeven aan het ML-model of DL-model\n",
        "        dat wordt gebruikt om te voorspellen\n",
        "\n",
        "    suffix : str\n",
        "        De 'tag' voor de naam van het csv\n",
        "        bestand, zodat deze makkelijk te identificeren\n",
        "        is na de submission.\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "    None\n",
        "        In plaats van een return maakt het een bestand\n",
        "        aan in de map Kaggle Submissions. De namen zien\n",
        "        er als volgt uit:\n",
        "\n",
        "    \"\"\"\n",
        "    # Aanmaken van de voorspelling\n",
        "    y_pred = model.predict(input_)\n",
        "\n",
        "    # Aanmaken df met alleen House ID en Price\n",
        "    test_predictions_df = pd.DataFrame(\n",
        "        {'House ID': test_data['House ID'],\n",
        "        'Price': np.ravel(y_pred)})\n",
        "\n",
        "    # Aanmaken van tijd\n",
        "    tijd = datetime.datetime.now().strftime(\"%m%d%H%M%S\")\n",
        "\n",
        "    # Aanmaken csv bestand met timestamp\n",
        "    test_predictions_df.to_csv(\n",
        "        f'Kaggle/voorspelling_{suffix}_{tijd}.csv',\n",
        "        index=False)\n",
        "\n",
        "    # Print voor conformatie\n",
        "    print(f'voorspelling_{suffix}_{tijd}.csv has been saved!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-rTcHFNhfkI"
      },
      "source": [
        "Nu zal de functie worden uitgevoerd om de voorspellingen te verkrijgen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnOhyFz-hfkI"
      },
      "outputs": [],
      "source": [
        "# Uitvoeren van kaggle_csv functie\n",
        "kaggle_csv(fcnn, sc_test_fe, 'FCNN-2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l8Ma262mzkV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlBDUE-nhfkJ"
      },
      "source": [
        "- [ ] Bouw en train een dense (fully-connected) netwerk om de huizenprijzen te voorspellen op basis van de tabular data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe6fCyfvhfkK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QBnM5kAhfkK"
      },
      "source": [
        "- [ ] Beschrijf welke lossfunctie het meest geschikt is voor dit probleem. Kies een geschikte optimizer en beschrijf kort hoe deze optimizer werkt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_NhVCFYhfkL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojoNTtuphfkM"
      },
      "source": [
        "- [ ] Plot je modelarchitectuur met behulp van een geschikte visualisatietool.\n",
        "- [ ] Onderbouw je keuzes van het aantal convolutionele lagen, poollagen, convolutionele kernels per laag, het aantal fully-connected lagen en het aantal neuronen per laag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBFbu5K1hfkM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQgjXNukhfkN"
      },
      "source": [
        "## CNN from Scratch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDN9UjsihfkV"
      },
      "source": [
        "- [ ] Bouw een convolutioneel neuraal netwerkmodel om de prijzen te voorspellen op basis van de afbeeldingen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1Hvs4TyW1pm"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  \"Train\",\n",
        "  seed=42,\n",
        "  label_mode=None,\n",
        "  image_size=(256, 256),\n",
        "  batch_size=4)\n",
        "\n",
        "# Extract file paths from the dataset\n",
        "image_file_paths = train_ds.file_paths\n",
        "\n",
        "# Extract only file ID without extension from file paths\n",
        "image_ids = [int(Path(image_file_path).stem) for image_file_path in image_file_paths]\n",
        "\n",
        "# Map each file path to its label using the DataFrame\n",
        "# Ensure the uuids in the DataFrame exactly match those in `image_uuids`\n",
        "labels = [train_data.loc[train_data['House ID'] == hid, 'Price'].values for hid in image_ids]\n",
        "\n",
        "# Convert the list of labels into a TensorFlow dataset\n",
        "labels_dataset = tf.data.Dataset.from_tensor_slices(labels).batch(4)\n",
        "\n",
        "# Combine the original image dataset with the labels dataset\n",
        "final_dataset = tf.data.Dataset.zip((train_ds, labels_dataset))\n",
        "\n",
        "train_dataset = final_dataset.take(110)\n",
        "val_dataset = final_dataset.skip(110)\n",
        "\n",
        "# Define a function to normalize the images\n",
        "def normalize_img(image, label):\n",
        "    # Normalize the pixel values to the range [0, 1]\n",
        "    return tf.cast(image, tf.float32) / 255.0, label\n",
        "\n",
        "# Apply normalization to both training and validation datasets\n",
        "train_dataset_normalized = train_dataset.map(normalize_img)\n",
        "val_dataset_normalized = val_dataset.map(normalize_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsQHBQWoYCuW"
      },
      "outputs": [],
      "source": [
        "img_shape = (256, 256, 3)\n",
        "batch_size = 50\n",
        "epochs = 5000\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "loss = tf.keras.losses.MeanSquaredError()\n",
        "metrics = [tf.keras.metrics.MeanAbsolutePercentageError()]\n",
        "\n",
        "cnn_model = Sequential([\n",
        "    keras.Input(shape=img_shape),\n",
        "    Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, 'linear')\n",
        "])\n",
        "\n",
        "cnn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3L0QmMDYFkQ"
      },
      "outputs": [],
      "source": [
        "cnn_model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=metrics\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAMaLt4JYG8u"
      },
      "outputs": [],
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "\n",
        "cnn_history = cnn_model.fit(\n",
        "    train_dataset_normalized,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_dataset_normalized,\n",
        "    callbacks=[early_stop]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPilKUG8hfkX"
      },
      "source": [
        "- [ ] Plot je modelarchitectuur met behulp van een geschikte visualisatietool.\n",
        "- [ ] Onderbouw je keuzes van het aantal lagen, het aantal neuronen per laag, de activeringsfuncties, de keuze van optimizer en het aantal trainingsepochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8W3Gs4qhfkX"
      },
      "outputs": [],
      "source": [
        "#Plot modelarchitecture\n",
        "keras.utils.plot_model(cnn_model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yMUNxIKl55h"
      },
      "source": [
        "## Keuzes van het model\n",
        "\n",
        "### De lagen\n",
        "\n",
        "1. **Inputlaag**: Dit is waar de gegevens binnenkomen. Het model verwacht afbeeldingen van 512 bij 512 pixels met 3 kleurkanalen.\n",
        "\n",
        "2. **Convolutionele lagen (Conv2D)**: Deze lagen zijn als filters die door de afbeeldingen gaan om verschillende kenmerken te vinden, zoals randen, texturen en vormen. We gebruiken drie van deze lagen om op verschillende niveaus van detail te leren.\n",
        "\n",
        "3. **Pooling-lagen (MaxPooling2D)**: Deze lagen verminderen de grootte van de gegevens door de resolutie te verkleinen, waardoor de complexiteit van het model afneemt en overfitting wordt voorkomen.\n",
        "\n",
        "4. **Flatten-laag**: Hier worden de gegevens omgevormd van een 3D-structuur naar een eenvoudige lijst, zodat ze kunnen worden gebruikt in de volgende lagen.\n",
        "\n",
        "5. **Dense (volledig verbonden) lagen**: Dit zijn lagen waar elk neuron verbonden is met elk neuron in de vorige laag. Ze leren complexere patronen en worden vaak gebruikt om de uiteindelijke voorspellingen te maken.\n",
        "\n",
        "### Aantal neuronen per laag\n",
        "\n",
        "### De actervingsfuncties\n",
        "\n",
        "1. Voor Conv2D wordt de ReLU-activatiefunctie gebruikt, aangeduid met 'relu'. ReLU zet negatieve invoerwaarden om in nul en laat positieve waarden ongewijzigd, waardoor het model niet-lineaire kenmerken kan leren.\n",
        "\n",
        "2. Voor de laatste dense laag wordt een lineaire activatiefunctie gebruikt. Voor prijsvoorspellingen op basis van afbeeldingen is het het beste om een lineaire activatiefunctie te gebruiken voor de laatste laag van het model. Dit past goed bij regressietaken waarbij het model een continue uitvoer moet voorspellen.\n",
        "\n",
        "### Keuze van Adam optimizer\n",
        "\n",
        "Goed voor veel variatie in de data: Adam past de leersnelheid aan voor elk onderdeel van het model, wat handig is als de gegevens veel variatie hebben.\n",
        "\n",
        "Combineert voordelen: Het combineert de beste eigenschappen van twee andere optimizers (AdaGrad en RMSProp), waardoor het beter kan werken dan oudere methoden zoals SGD.\n",
        "\n",
        "Minder instellingen nodig: Je hoeft niet zoveel handmatige instellingen te doen als bij andere methoden. Dit voorkomt veel tijd. (Brownlee, 2021)\n",
        "\n",
        "Brownlee, J. (2021, January 12). Gentle introduction to the ADAM Optimization Algorithm for deep learning. MachineLearningMastery.com. https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
        "\n",
        "\n",
        "### Keuze van aantal epochs\n",
        "\n",
        "Wij hebben gekozen voor 5000 epochs om zo het mode met zekerheid te optimaliseren. We hebben van gebruikt gemaakt van early stopping om zo dagen aan tijd te besparen en overfitting te voorkomen.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK7q2CUthfkY"
      },
      "source": [
        "- [ ] Gebruik een of meer van de volgende technieken in je model. Beschrijf en motiveer je keuze:\n",
        "    - [ ] L1 regularisatie\n",
        "    - [ ] L2 regularisatie\n",
        "    - [ ] Dropout\n",
        "    - [ ] Batchnorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k9JS8sqhfkY"
      },
      "outputs": [],
      "source": [
        "# Use L2 and dropout in the model\n",
        "\n",
        "# cnn_model2 = Sequential([\n",
        "#     keras.Input(shape=img_shape),\n",
        "#     Conv2D(filters=32, kernel_size=(3,3), activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "#     MaxPooling2D((2,2)),\n",
        "#     Conv2D(filters=64, kernel_size=(3,3), activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "#     MaxPooling2D((2,2)),\n",
        "#     Conv2D(filters=64, kernel_size=(3,3), activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "#     Flatten(),\n",
        "#     Dense(64, activation='relu'),\n",
        "#     Dropout(0.2),\n",
        "#     Dense(1, 'linear')\n",
        "# ])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cnn_model2.compile(\n",
        "#     optimizer=optimizer,\n",
        "#     loss=loss,\n",
        "#     metrics=metrics\n",
        "#     )"
      ],
      "metadata": {
        "id": "b7P8lOpMXnQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cnn_history2 = cnn_model2.fit(\n",
        "#     train_dataset_normalized,\n",
        "#     batch_size=batch_size,\n",
        "#     epochs=epochs,\n",
        "#     validation_data=val_dataset_normalized,\n",
        "#     callbacks=[early_stop]\n",
        "# )"
      ],
      "metadata": {
        "id": "HoGtGpxaXp-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-WSOcVxl55h"
      },
      "source": [
        "### Motivatie\n",
        "\n",
        "Het combineren van L2-regularisatie en dropout kan een krachtige manier zijn om overfitting te verminderen en de algehele prestaties van het model te verbeteren. Door de gewichten te reguleren met L2-regularisatie en tegelijkertijd het model robuuster te maken met dropout, kan het model beter generaliseren naar nieuwe gegevens en nauwkeurigere voorspellingen doen. Dit kan vooral nuttig zijn bij het voorspellen van prijzen op basis van afbeeldingen, waar het belangrijk is om de relevante kenmerken van de afbeeldingen vast te leggen zonder te veel te vertrouwen op specifieke details die alleen relevant zijn voor de trainingsgegevens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqHxCFtWhfkZ"
      },
      "source": [
        "- [ ] Beschrijf welke vormen van data augmentation (indien van toepassing) geschikt zijn voor dit probleem. Implementeer dit in je datapijplijn.\n",
        "- [ ] Compile je model met je gekozen lossfunctie en optimizer, en train deze op de afbeeldingen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXHEBXivXOyO"
      },
      "source": [
        "**Augmentation**  \n",
        "- Padding: maakt extra pixels om de gehele image, dit wordt gebruikt om zo veel mogelijk informatie te behouden.  \n",
        "- Stride: geeft aan met hoeveel pixels de kernel zich verschuift op de image. In het geval dat er niet genoeg pixels over zijn, bijvoorbeeld er is maar 1 row van pixels over na het shiften, dan gaat de hele row niet gebruikt worden.\n",
        "- MaxPooling: pakt het hoogste waarde uit de kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhSaUX_Uhfka"
      },
      "outputs": [],
      "source": [
        "# kernel_regularizer = keras.regularizers.l1(0.01)\n",
        "\n",
        "cnn_model3 = Sequential([\n",
        "    keras.Input(shape=img_shape),\n",
        "\n",
        "    Conv2D(filters=32, kernel_size=(3,3), activation=None, padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "\n",
        "    Conv2D(filters=64, kernel_size=(3,3), activation=None, padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "\n",
        "    Conv2D(filters=64, kernel_size=(3,3), activation=None, padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Activation('relu'),\n",
        "\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, 'linear')\n",
        "])\n",
        "\n",
        "cnn_model3.summary()\n",
        "\n",
        "cnn_model3.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=metrics\n",
        "    )\n",
        "\n",
        "cnn_history3 = cnn_model3.fit(\n",
        "    train_dataset_normalized,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_dataset_normalized,\n",
        "    callbacks=[early_stop]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYjebuMMhfkb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LYWisaihfkc"
      },
      "source": [
        "# Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ihCVtVhhfkc"
      },
      "source": [
        "# Multimodaal model"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iO0qPPUBZCKu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}